"Authors","Author full names","Author(s) ID","Title","Year","Source title","Volume","Issue","Art. No.","Page start","Page end","Cited by","DOI","Link","Abstract","Author Keywords","Index Keywords","Document Type","Publication Stage","Open Access","Source","EID"
"Brunnert, A.","Brunnert, Andreas (8561765400)","8561765400","Evaluating the Accuracy of Software Energy Consumption Models for Java Applications at Process and Transaction Levels","2025","Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering","","","","1441","1448","0","10.1145/3696630.3728709","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105013970670&doi=10.1145%2F3696630.3728709&partnerID=40&md5=c9a0c8000c81c2c414b0aed6f42cfd79","The energy consumption of software systems is hard to quantify because software does not consume energy itself but rather the hardware that it runs on. To address this challenge, technologies such as the Intel Running Average Power Limit (RAPL) interface have been developed to measure the energy consumed by various hardware resources at runtime. Additionally, specialized tools like JoularJX have been created to attribute RAPL measurements to individual software systems and their components. However, these tools are typically limited to software running directly on a server, without virtualization or containerization. In virtualized or containerized environments, direct energy measurements are only possible if the hypervisor or container runtime forwards this information to the guests-a practice not widely adopted, particularly in cloud environments. In such cases, energy consumption models are used to estimate software energy usage based on other metrics, such as CPU, memory, storage, or network utilization. One recently released tool following this approach is the OpenTelemetry Java-Agent extension (OTJAE), making it suitable for environments where direct measurements are not feasible. This study aims to evaluate the accuracy of JoularJX and OTJAE, along with their underlying models, in calculating energy consumption at both the process and transaction levels of Java applications. Both models under examination are based on CPU demand measurements. We are able to show that the model predictions match actual measurements obtained from a hardware server with high accuracy in medium and high load scenarios (50% to 100% CPU utilization) but vary significantly for lower load levels. © 2025 Copyright held by the owner/author(s).","Green IT; Green Software Engineering; Software Energy Consumption","Application programs; Computer hardware; Containers; Energy utilization; Green computing; Software agents; Virtual reality; Virtualization; Energy; Energy consumption model; Energy-consumption; Green software engineering; Green-IT; Java applications; Process levels; Software energy consumption; Software-systems; Transaction level; Java programming language","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-105013970670"
"Kondratyev, D.; Riedel, B.; Chou, Y.-T.; Cochran-Branson, M.; Paladino, N.; Schultz, D.; Liu, M.; Duarte, J.; Harris, P.; Hsu, S.-C.","Kondratyev, Dmitry (57224311554); Riedel, Benedikt (57197584958); Chou, Yuantang (57196418475); Cochran-Branson, Miles G. (59537177100); Paladino, Noah (57219546217); Schultz, David (56438840400); Liu, Mia H. (58367490500); Duarte, Javier Mauricio (54392849500); Harris, Philip Coleman (57208119901); Hsu, Shih Chieh Hieh (55769746824)","57224311554; 57197584958; 57196418475; 59537177100; 57219546217; 56438840400; 58367490500; 54392849500; 57208119901; 55769746824","SuperSONIC: Cloud-Native Infrastructure for ML Inferencing","2025","","","","29","","","1","10.1145/3708035.3736049","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105013078215&doi=10.1145%2F3708035.3736049&partnerID=40&md5=f9293cca7634af81144ea8708b3248dd","The increasing computational demand from growing data rates and complex machine learning (ML) algorithms in large-scale scientific experiments has driven the adoption of the Services for Optimized Network Inference on Coprocessors (SONIC) approach. SONIC accelerates ML inference by offloading it to local or remote coprocessors to optimize resource utilization. Leveraging its portability to different types of coprocessors, SONIC enhances data processing and model deployment efficiency for cutting-edge research in high energy physics (HEP) and multi-messenger astrophysics (MMA). We developed the SuperSONIC project, a scalable server infrastructure for SONIC, enabling the deployment of computationally intensive tasks to Kubernetes clusters equipped with graphics processing units (GPUs). Using NVIDIA Triton Inference Server, SuperSONIC decouples client workflows from server infrastructure, standardizing communication, optimizing throughput, load balancing, and monitoring. SuperSONIC has been successfully deployed for the CMS and ATLAS experiments at the CERN Large Hadron Collider (LHC), the IceCube Neutrino Observatory (IceCube), and the Laser Interferometer Gravitational-Wave Observatory (LIGO) and tested on Kubernetes clusters at Purdue University, the National Research Platform (NRP), and the University of Chicago. SuperSONIC addresses the challenges of the Cloud-native era by providing a reusable, configurable framework that enhances the efficiency of accelerator-based inference deployment across diverse scientific domains and industries. © 2025 Copyright held by the owner/author(s)","heterogeneous computing; inference as a service; machine learning","Astrophysics; Balancing; Cloud computing; Cluster computing; Complex networks; Computer graphics; Computer software reusability; Cutting; Gravitation; Green computing; High energy physics; Inference engines; Learning algorithms; Machine learning; Program processors; Co-processors; Complex machines; Computational demands; Data-rate; Heterogeneous computing; Ice cubes; Inference as a service; Machine-learning; Network inference; Server infrastructure; Graphics processing unit","Conference paper","Final","All Open Access; Green Accepted Open Access; Green Open Access","Scopus","2-s2.0-105013078215"
"Haddad, T.; Kumarapeli, P.","Haddad, Tia (59297681500); Kumarapeli, Pushpa (13907399900)","59297681500; 13907399900","A Sustainable Approach to Processing Big Healthcare Data (BHD) Workloads","2025","Studies in Health Technology and Informatics","327","","","775","776","0","10.3233/SHTI250463","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005816714&doi=10.3233%2FSHTI250463&partnerID=40&md5=45156a61477169a03d6e39181b4a45a3","The growing volume of Big Healthcare Data (BHD) workloads requires sustainable solutions to minimise environmental impact. This poster presents a pilot architectural framework that leverages Microservices Architecture (MSA) to enhance the sustainability of BHD workflows. The framework optimises cloud scaling strategies by integrating energy-efficient models with autoscaling tools, such as Kubernetes Event-driven Autoscaling (KEDA) and Horizontal Pod Autoscaling (HPA). © 2025 The Authors.","Big Healthcare Data; Microservices Architecture Systems; Sustainable Healthcare Systems; Sustainable Software Engineering","Architecture; Big data; Computer architecture; Environmental impact; Green computing; Software engineering; Sustainable development; Architectural frameworks; Autoscaling; Big healthcare data; Healthcare systems; Microservice architecture system; Scalings; Sustainable healthcare system; Sustainable softwares; Sustainable solution; Work-flows; Energy efficiency; clinical article; conference paper; environmental impact; health care system; health data; human; human cell; pilot study; software; workflow; workload; big data; cloud computing; Big Data; Cloud Computing; Humans; Pilot Projects; Workflow; Workload","Conference paper","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-105005816714"
"Sarabu, V.R.","Sarabu, Venkata Rahul (59511098300)","59511098300","Optimizing machine learning and deep learning software stacks for AI-specific cloud infrastructures","2025","","","","","431","478","0","10.4018/979-8-3693-9694-0.ch021","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004780872&doi=10.4018%2F979-8-3693-9694-0.ch021&partnerID=40&md5=2cc682b1648c1f568490f55ec3d1b84b","Advancements in Artificial Intelligence (AI), particularly in Machine Learning (ML) and Deep Learning (DL), have driven innovation across industries like healthcare, finance, and education, necessitating robust, scalable cloud infrastructures to handle complex AI workloads (Zhang, Cheng, & Boutaba, 2010). This chapter examines the critical components of an optimized ML/DL software stack for AI-specific cloud environments, including frameworks, libraries, development kits, and cloud management software. It also explores advanced technologies like containerization, orchestration, and serverless computing, which ensure scalability and efficiency. Additionally, strategies for optimizing performance, improving energy efficiency, and enhancing workload distribution are discussed, with a focus on seamless integrationand operational cost-efficiency. The chapter offers practical insights into designing AI-specific cloud infrastructures that meet the computational demands of modern AI applications. © 2025 by IGI Global Scientific Publishing. All rights reserved.","","Advanced technology; Cloud environments; Cloud infrastructures; Cloud managements; Critical component; Learning software; Machine-learning; Management software; Optimizing performance; Software stacks; Deep learning","Book chapter","Final","","Scopus","2-s2.0-105004780872"
"Rukmini, S.; Soma, S.","Rukmini, S. (57961341300); Soma, Shridevi (57202212357)","57961341300; 57202212357","Enhanced beluga whale optimization-based container migration between virtual machines","2025","Multimedia Tools and Applications","84","14","","13691","13715","1","10.1007/s11042-024-19484-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004012390&doi=10.1007%2Fs11042-024-19484-2&partnerID=40&md5=8930411a5d489fa51316aaa6ecc02246","Container technology represents a dynamic and adaptable solution in the field of software development for cloud computing applications and services, presenting the advantages of both portability and operational efficiency. Cloud computing environments often require robust support for high-performance computing, necessitating the use of containers, which are similar to Virtual Machines (VMs) but more lightweight. Container placement within data centers is a critical fact of cloud computing, prompting research efforts in the field of service computing. In this research work an innovative approach, the Enhanced Beluga Whale Optimization (EBWO) based container migration between VM, designed to optimize energy consumption and load balancing in computing environments. The primary goal is to minimize energy consumption, especially during periods of high VM resource utilization when traditional optimization techniques tend to reduce in performance. To achieve this, the research leverages the EBWO for container consolidation, resulting in reduced energy consumption. Container migration techniques are utilized, allowing the seamless transfer of services from one VM to another, aided by the CRIU tool within CentOS 7. VM are created efficiently using the Ansible tool on IBM servers. The performance method attains load 0.0019 (L), migration cost 0.0406(ms), energy consumption 0.0453(kWh), no. of VM migration 2876(vm), SLA violations 2876(%), energy SLA violation 0.009(%), computational time 2.50(sec) and resources availability 0.9955 (%). © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2024.","Cloud computing; Container consolidation; Container migration; Enhanced Beluga Whale Optimization; Virtual machine","Application programs; Balancing; Cloud computing; Computer software portability; Containers; Energy utilization; Green computing; Network security; Software design; Beluga whales; Cloud-computing; Computing applications; Computing services; Container consolidation; Container migration; Energy-consumption; Enhanced beluga whale optimization; Optimisations; Performance; Virtual machine","Article","Final","","Scopus","2-s2.0-105004012390"
"Okafor, K.C.; Okafor, W.O.; Mary Longe, O.M.; Ignatius Ayogu, I.I.; Anoh, K.; Adebisi, B.","Okafor, Kennedy Chinedu (56728462700); Okafor, Wisdom O. (57469799800); Mary Longe, Omowunmi Mary (55026419700); Ignatius Ayogu, Ikechukwu (57202096868); Anoh, Kelvin O.O. (56029788200); Adebisi, B. (21833798800)","56728462700; 57469799800; 55026419700; 57202096868; 56029788200; 21833798800","Scalable Container-Based Time Synchronization for Smart Grid Data Center Networks","2025","Technologies","13","3","105","","","3","10.3390/technologies13030105","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001171454&doi=10.3390%2Ftechnologies13030105&partnerID=40&md5=53921deb6125429460beb9fe9c3b142c","The integration of edge-to-cloud infrastructures in smart grid (SG) data center networks requires scalable, efficient, and secure architecture. Traditional server-based SG data center architectures face high computational loads and delays. To address this problem, a lightweight data center network (DCN) with low-cost, and fast-converging optimization is required. This paper introduces a container-based time synchronization model (CTSM) within a spine–leaf virtual private cloud (SL-VPC), deployed via AWS CloudFormation stack as a practical use case. The CTSM optimizes resource utilization, security, and traffic management while reducing computational overhead. The model was benchmarked against five DCN topologies—DCell, Mesh, Skywalk, Dahu, and Ficonn—using Mininet simulations and a software-defined CloudFormation stack on an Amazon EC2 HPC testbed under realistic SG traffic patterns. The results show that CTSM achieved near-100% reliability, with the highest received energy data (29.87%), lowest packetization delay (13.11%), and highest traffic availability (70.85%). Stateless container engines improved resource allocation, reducing administrative overhead and enhancing grid stability. Software-defined Network (SDN)-driven adaptive routing and load balancing further optimized performance under dynamic demand conditions. These findings position CTSM-SL-VPC as a secure, scalable, and efficient solution for next-generation smart grid automation. © 2025 by the authors.","cloud computing; containerization; data center network; smart grid; software-defined network","Benchmarking; Binary decision diagrams; Cloud platforms; Grid computing; Plastic bottles; Trees (mathematics); Cloud infrastructures; Cloud-computing; Containerization; Data center networks; Grid data; Private clouds; Smart grid; Software-defined networks; Synchronization models; Time synchronization; Resource allocation","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-105001171454"
"","","","11th IFIP WG 6.12 European Conference on Service-Oriented and Cloud Computing, ESOCC 2025","2025","Lecture Notes in Computer Science","15547 LNCS","","","","","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000016227&partnerID=40&md5=eb8bb94d192da2ed24cae15bf3bab2fc","The proceedings contain 18 papers. The special focus in this conference is on Service-Oriented and Cloud Computing. The topics include: SemT: A Framework for Enhancing Tabular Data Through Enrichment-as-a-Service; towards WebAssembly-Based Federated Learning; a Bio-inspired Leader-Based Energy Management System for Drone Fleets; carbon-Aware Software Services; comparative Analysis of Lightweight Kubernetes Distributions for Edge Computing: Performance and Resource Efficiency; comparative Analysis of Lightweight Kubernetes Distributions for Edge Computing: Security, Resilience and Maintainability; Enhancing Failure Resilience of Cloud-Edge Microservices: The FREEDA Approach; ML-Based Performance Modeling in Edge FaaS Systems; a Quantitative Privacy Evaluation Method Based on Tsallis Entropy for Trustworthy Data Sharing; SafeAR: Privacy-Maintenance in Augmented Reality Applications; deep Surrogate Models of Serverless Batch Processing Services; pyStorageLess: Leveraging Von Neumann’s Architecture to Abstract Storage Heterogeneity in Serverless Applications; A Conceptual Framework for API Refactoring in Service-Oriented Architectures; a Survey Study About the Impacts of Introducing a Microservices Cataloging Tool in a Large Software Development Unit; TOSCARISMA: Modeling CARISMA-Based Service Communication Using TOSCA; workflow Net Compositions for the Analysis of Service-Oriented Systems.","","","Conference review","Final","","Scopus","2-s2.0-86000016227"
"Lozinskyi, A.; Gladun, A.","Lozinskyi, Anatol (60139281600); Gladun, Anatoly Ya (25823057300)","60139281600; 25823057300","Optimizing the Energy Consumption of On-site Private Cloud Computing Platforms","2025","CEUR Workshop Proceedings","4049","","","91","100","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105018671249&partnerID=40&md5=198599aa87596e3e8306ff69860413ad","Cloud computing offers advantages such as self-service and dynamic redistribution of resource structures, necessitating the development of effective algorithms for optimal use of hardware infrastructure. Recent studies highlight the problems of resource redistribution in on-site private cloud computing platforms software layers, affecting total energy consumption. Existing approaches to mathematical modeling use combinatorial optimization, game theory, and artificial intelligence but lack an integrated approach to energy consumption optimization. The proposed approach involves creating an original algorithm based on a multi-level architecture developed by the author. This architecture divides objects into seven functional layers, from hardware physical infrastructure to application SaaS layer. The algorithm aims to minimize electricity consumption by isolating critical parameters in the process of redistributing resource structures. The optimization problem is segmented into subproblems, allowing for efficient load placement and energy savings. The model is focused on optimizing the power consumption of on-site private cloud computing platforms using a well-known approach - reducing the number of hardware nodes involved. The idea is to switch off not only individual computing nodes but also cooling nodes when idle. The mathematical formulation of the resource allocation in on-site private cloud computing platforms problem is presented as a Multi-dimensional Bin packing. The algorithm employs dynamic programming to maximize load placement density and reduce the number of hardware nodes. The model provides support technologies for both virtual machines and software containers. The paper presents the original author's algorithm, Optimization of Programmable Infrastructure Resources (OPIR), focused on reducing electricity consumption through combinatorial optimization. © 2025 Copyright for this paper by its authors.","cloud computing; energy optimization; multi-layer architecture of system; on-site private cloud computing platforms; resource allocation","Cloud computing; Combinatorial optimization; Computation theory; Computer hardware; Dynamic programming; Dynamics; Electric power utilization; Energy efficiency; Game theory; Green computing; Memory architecture; Optimal systems; Structural optimization; Cloud computing platforms; Cloud-computing; Electricity-consumption; Energy optimization; Energy-consumption; Multi-layer architecture of system; Multi-layer architectures; On-site private cloud computing platform; Private clouds; Resources allocation; Resource allocation","Conference paper","Final","","Scopus","2-s2.0-105018671249"
"Falloon, M.; Ma, H.; Chen, G.","Falloon, Mathew (59374152700); Ma, Hui (55723485400); Chen, Gang (57226177152)","59374152700; 55723485400; 57226177152","Energy-Aware Resource Allocation and Container Migration in Distributed Data Centers Under Variable Energy Pricing: A Genetic Programming Hyper-Heuristic Approach","2025","IEEE International Conference on Cloud Computing, CLOUD","","","","233","242","0","10.1109/CLOUD67622.2025.00032","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105016015557&doi=10.1109%2FCLOUD67622.2025.00032&partnerID=40&md5=7903df48968fa5c144272f3de65d228b","Containers have emerged as a prevalent mechanism for deploying software applications within cloud data centers, thereby abstracting many operational details from developers and transferring the responsibility of resource management to cloud service providers. These providers are continually motivated to reduce operational costs by optimizing both the placement of containers and the Virtual Machines (VMs) that host them, as efficient placement directly contributes to reduced energy expenditures. Furthermore, given that energy prices vary both temporally and geographically due to fluctuating power production, demand, and the increasing integration of renewable en-ergy sources, explicit consideration of dynamic, location-specific energy pricing is essential for informed resource allocation and container migration decisions. In this work, we propose a novel container-based cloud resource allocation model that integrates variable energy prices across multiple locations. To address this problem, we introduce an innovative Genetic Programming Hyper-Heuristic (GPHH) algorithm that concurrently evolves three heuristics for container allocation, VM placement, and container migration. A key technical novelty of our approach is the incorporation of newly designed terminals within the GPHH framework that are specifically engineered to capture and utilize the complex dynamic power pricing information. Experimental results demonstrate that our GPHH algorithm offers significant improvements over several state-of-the-art methods, thereby enhancing both cost efficiency and energy optimization in cloud environments. © 2025 IEEE.","Cloud resource allocation; Container migration; Cost optimization strategies; Genetic Programming; Green cloud solutions; Grid-aware system management","Cloud computing; Containers; Cost reduction; Distributed database systems; Energy efficiency; Genetic algorithms; Green computing; Heuristic algorithms; Heuristic methods; Information management; Power management; Resource allocation; Cloud resource allocation; Container migration; Cost optimization strategy; Costs Optimization; Green cloud solution; Green Clouds; Grid-aware system management; Optimization strategy; Resources allocation; Systems management; Genetic programming","Conference paper","Final","","Scopus","2-s2.0-105016015557"
"Belhadj, M.A.; Trabelsi, K.; Cudennec, L.; Charles, H.-P.","Belhadj, Mohamed Anisse (60092992900); Trabelsi, Kods (57217107475); Cudennec, Loïc (15065052000); Charles, Henri Pierre (24476273000)","60092992900; 57217107475; 15065052000; 24476273000","Software Container-based Energy Estimation Models for ARM Architecture","2025","","","","","1132","1139","0","10.1109/IPDPSW66978.2025.00177","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105015550698&doi=10.1109%2FIPDPSW66978.2025.00177&partnerID=40&md5=57a05eb7636ac9da4f61850294ef550b","The exponential data growth from billions of sensors and connected devices has intensified the need for efficient energy management in embedded ARM-based systems. Traditional power estimation models primarily rely on CPU utilization, neglecting other critical factors like memory and network activity, which reduces accuracy. This paper proposes a fine-grained power estimation approach at the process and container level, employing machine-learning models trained on high-frequency monitoring of CPU, memory, and network usage through Performance Monitoring Counters (PMCs).Multiple machine-learning models, including XGBoost, Random Forest, and Bayesian Ridge Regression, are developed and evaluated across diverse real-world workloads to improve energy consumption predictions. Our findings show that XGBoost achieves the highest accuracy (MAPE 9.14%), while Bayesian Ridge Regression provides the fastest inference time, making it well-suited for real-time energy-aware decision-making. Additionally, our method enables precise separation of energy consumption at the process and container level, offering a more profound understanding of power distribution. These advancements support energy-efficient container scheduling and workload placement, enabling better resource management in edge-to-cloud environments and enhancing the overall sustainability of distributed computing systems. © 2025 IEEE.","ARM Architecture; Container; Performance Monitoring Counters; Power Consumption; Prediction models","Architecture; ARM processors; Bayesian networks; Cloud computing; Decision making; Embedded systems; Energy efficiency; Energy utilization; Green computing; Learning systems; Machine learning; Memory architecture; Network architecture; Power management; ARM architecture; Bayesian; Estimation models; Machine learning models; Performance monitoring counter; Performance-monitoring; Power; Power estimations; Prediction modelling; Ridge regression; Containers","Conference paper","Final","","Scopus","2-s2.0-105015550698"
"Singh, A.K.; Saxena, D.; Lindenstruth, V.","Singh, Ashutosh Kumar (57212846086); Saxena, Deepika (57189319379); Lindenstruth, Volker (55031708600)","57212846086; 57189319379; 55031708600","REE-TM: Reliable and Energy-Efficient Traffic Management Model for Diverse Cloud Workloads","2025","IEEE Transactions on Cloud Computing","13","3","","953","968","0","10.1109/TCC.2025.3581697","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008915732&doi=10.1109%2FTCC.2025.3581697&partnerID=40&md5=d4eca044f9fbe101eb4b13f009a529c2","Diversity of workload demands lays a critical impact on efficient resource allocation and management of cloud services. The existing literature has either weakly considered or overlooked the heterogeneous feature of job requests received from wide range of internet services users. To address this context, the proposed approach named Reliable and Energy Efficient Traffic Management (REE-TM) has exploited the diversity of internet traffic in terms of variation in resource demands and expected complexity. Specifically, REE-TM incorporates categorization of heterogeneous job requests and executes them by selecting the most admissible virtual node (a software-defined instance such as a virtual machine or container) and physical node (an actual hardware server or compute host) within the cloud infrastructure. To deal with resource-contention-based resource failures and performance degradation, a novel workload estimator ‘Toffoli Gate-based Quantum Neural Network’ (TG-QNN) is proposed, wherein learning process or interconnection weights optimization is achieved using Quantum version of BlackHole (QBHO) algorithm. The proactively estimated workload is used to compute entropy of the upcoming internet traffic with various traffic states analysis for detection of probable resource-congestion. REE-TM is extensively evaluated through simulations using a benchmark dataset and compared with optimal and without REE-TM versions. The performance evaluation and comparison of REE-TM with measured significant metrics reveal its effectiveness in assuring higher reliability by up to 30.25% and energy-efficiency by up to 23% as compared without REE-TM. © 2013 IEEE.","energy-efficiency; Internet services; physical nodes; resource utilization; virtual nodes","Cloud computing; Green computing; Resource allocation; Traffic congestion; Efficient resource allocation; Energy; Energy efficient; Internet traffic; Internet-services; Management Model; Physical nodes; Resources utilizations; Traffic management; Virtual node; Energy efficiency","Article","Final","","Scopus","2-s2.0-105008915732"
"Werner, S.; Borges, M.C.; Wolf, K.; Tai, S.","Werner, Sebastian (56354164400); Borges, Maria C. (57215309343); Wolf, Karl (57216289009); Tai, Stefan (7102829484)","56354164400; 57215309343; 57216289009; 7102829484","A Comprehensive Experimentation Framework for Energy-Efficient Design of Cloud-Native Applications","2025","","","","","176","186","2","10.1109/ICSA65012.2025.00026","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005027472&doi=10.1109%2FICSA65012.2025.00026&partnerID=40&md5=e81f16b44efabd09fc9cd0ebec4120d0","Current approaches to designing energy-efficient applications typically rely on measuring individual components using readily available local metrics, like CPU utilization. However, these metrics fall short when applied to cloud-native applications, which operate within the multi-tenant, shared environments of distributed cloud providers. Assessing and optimizing the energy efficiency of cloud-native applications requires consideration of the complex, layered nature of modern cloud stacks.To address this need, we present a comprehensive, automated, and extensible experimentation framework that enables developers to measure energy efficiency across all relevant layers of a cloud-based application and evaluate associated quality trade-offs. Our framework integrates a suite of service quality and sustainability metrics, providing compatibility with any Kubernetes-based application. We demonstrate the feasibility and effectiveness of this approach through initial experimental results, comparing architectural design alternatives for a widely used open-source cloud-native application. © 2025 IEEE.","Cloud-Computing; Experiment-driven Software Design; Sustainable and Quality Engineering","Electronic design automation; Enterprise software; Open source software; Software quality; 'current; Cloud-computing; CPU utilization; Energy; Energy efficient; Energy-efficient design; Experiment-driven software design; Individual components; Quality engineering; Sustainable engineering; Software design","Conference paper","Final","","Scopus","2-s2.0-105005027472"
"Arzo, S.T.; Scotece, D.; Bassoli, R.; Devetsikiotis, M.; Foschini, L.; Fitzek, F.H.P.","Arzo, Sisay Tadesse (55979134900); Scotece, Domenico (57201698918); Bassoli, Riccardo (55598393900); Devetsikiotis, Michael (35598790300); Foschini, Luca (7004212533); Fitzek, Frank H. P. (9434654200)","55979134900; 57201698918; 55598393900; 35598790300; 7004212533; 9434654200","Softwarized and containerized microservices-based network management analysis with MSN","2024","Computer Networks","254","","110750","","","4","10.1016/j.comnet.2024.110750","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202591450&doi=10.1016%2Fj.comnet.2024.110750&partnerID=40&md5=867df86fdceefe040615fdc0c434b428","Microservice architecture is a service-oriented paradigm that enables the decomposition of cumbersome monolithic-based software systems. Using microservice design principles, it is possible to develop flexible, scalable, reusable, and loosely coupled software that could be containerized and deployed in a distributed edge/cloud environment. The flexible deployment of microservices in an edge environment increases system performance in terms due to dynamic service function placement and chaining possibly resulting in latency reduction, fault tolerance, scalability, efficient resource utilization, cost reduction, and energy consumption reduction. On the other hand, virtualization and containerization of microservices add processing and communication overheads. Therefore, to evaluate end-to-end microservices-based system performance, we need to have an end-to-end mathematical formulation of the overall microservice-based network system. Incorporating the virtualization overhead, here we provide end-to-end mathematical formulation considering system parameters: latency, throughput, computational resource usage, and energy consumption. We then evaluate the formulation in a testbed environment with the Microservice-based SDN (MSN) framework that decomposes the Software-defined Networking (SDN) controller in microservices with Docker Container. The final result validates the presented mathematical modeling of the system's dynamic behavior which can be used to design a microservice-based system. © 2024 The Author(s)","5G; 6G; Beyond 5G; Cloudification; Containerization; Microservice","Computer software reusability; Network function virtualization; Scalability; Service oriented architecture (SOA); 5g; 6g; Beyond 5g; Cloudification; Containerization; End to end; Energy-consumption; Microservice; Systems performance; Virtualizations; 5G mobile communication systems","Article","Final","All Open Access; Green Accepted Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85202591450"
"Bhukya, S.; Goud, P.S.; Yuvraj, K.; Harin, G.K.","Bhukya, Sreedhar (37074008700); Goud, P. Suraj (59449107100); Yuvraj, K. (59449107200); Harin, Goud K. (59449537200)","37074008700; 59449107100; 59449107200; 59449537200","A Deep Reinforcement Learning Framework for Optimized Container Scheduling and Load Balancing","2024","International Research Journal of Multidisciplinary Technovation","6","6","","182","196","1","10.54392/irjmt24614","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210549685&doi=10.54392%2Firjmt24614&partnerID=40&md5=12ab127f85d00b677bdf6cb004ee3659","Unlike VMs, containerization is a modern method for packaging and deploying software in distributed environments like the cloud. Containers are widely used due to their efficient software packaging and deployment. Efficient management of containers is crucial in dynamic cloud environments with heterogeneous infrastructure. Deep learning techniques are being applied to optimize resource utilization in cloud environments, including mapping containers to suitable nodes for energy conservation. However, the existing works on container scheduling have limitations like inability to cope with dynamic runtime scenarios. To overcome this problem, the aim of this paper is to design and implement a framework using deep reinforcement learning techniques to improve container scheduling and load balancing. The proposed algorithm, Reinforcement Learning based Container Scheduling (RLbCS), uses an action-reward iterative approach to optimize container scheduling. Experimental results showed that RLbCS outperformed existing methods, achieving a 92% success rate in placing containers and optimizing resource utilization. The proposed method can be integrated with cloud-based systems to automatically schedule containers for resource optimization and load balancing. © The Author(s) 2024.","Cloud Computing; Container Scheduling; Container Services; Deep Reinforcement Learning; Load Balancing","","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85210549685"
"Rabieyan, R.; Yahyapour, R.; Jahnke, P.","Rabieyan, Reza (51261431200); Yahyapour, Ramin (15066204200); Jahnke, Patrick (57188692099)","51261431200; 15066204200; 57188692099","Optimization of containerized application deployment in virtualized environments: a novel mathematical framework for resource-efficient and energy-aware server infrastructure","2024","Journal of Supercomputing","80","15","","22598","22630","1","10.1007/s11227-024-06304-5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196804753&doi=10.1007%2Fs11227-024-06304-5&partnerID=40&md5=1a5fc0fad5d144c5da54d0f26b1b5a86","This study addresses the critical need for effective resource management through software container migration in cloud data centers. It emphasizes its role in avoiding resource shortages and reducing energy consumption in cloud environments. This study introduces a novel multi-objective integer linear programming (ILP) approach for software container replacements, complemented by a specialized algorithm designed to migrate software containers between over- and underutilized hosts to enhance efficiency compared to traditional optimization methods. The simulation results demonstrate the algorithm's effectiveness, validating its potential for optimizing energy usage and resource allocation in cloud environments. Statistical analyses confirm the proposed model's and algorithm's superiority over benchmark approaches, highlighting their potential for enhancing resource management in cloud computing systems. © The Author(s) 2024.","Containerization; Software container replacement; Workload optimization","Application programs; Energy utilization; Green computing; Information management; Integer programming; Natural resources management; Power management; Resource allocation; Virtual reality; Application deployment; Cloud environments; Containerization; Mathematical frameworks; Optimisations; Resource management; Resource-efficient; Software container replacement; Virtualized environment; Workload optimizations; Containers","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85196804753"
"Falloon, M.; Ma, H.; Chen, A.","Falloon, Mathew (59374152700); Ma, Hui (55723485400); Chen, Aaron (57219511408)","59374152700; 55723485400; 57219511408","Energy-Aware Dynamic Resource Allocation and Container Migration in Cloud Servers: A Co-evolution GPHH Approach","2024","","","","","1219","1227","2","10.1145/3638529.3654070","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206887253&doi=10.1145%2F3638529.3654070&partnerID=40&md5=53711712f8bbf02b3b855824f0779cb4","Containers are a popular way of deploying software in cloud data centers. Containers are allocated to Virtual machines (VMs) which are allocated to Physical machines (PMs) within the data center. Since the resources required by containers often do not match those of VMs, where to allocate them must be decided. A poor solution can result in high energy costs. Many existing methods to solve this problem use heuristics which do not consider containers leaving the data center after being allocated. Some do consider migrating containers between VMs but few do for energy efficiency reasons. These overlooked aspects may lead to increased energy usage, particularly since studies have demonstrated that many containers run for only a brief duration. In this paper, we develop a model of the container-based cloud resource allocation problem that considers the energy impact of leaving and migrating containers. We then design a new Genetic Programming Hyper-Heuristic (GPHH) algorithm to jointly evolve three heuristics for container placement, VM placement and container migration control. We utilize newly designed terminals to ensure the effectiveness of our GPHH algorithm. Experiments have been conducted with results indicating that the heuristics evolved by our GPHH algorithm can achieve better performance compared to several state-of-the-art techniques. © 2024 Copyright is held by the owner/author(s). Publication rights licensed to ACM.","co-evolution; combinatorial optimization; genetic programming; timetabling and scheduling","Energy efficiency; Energy utilization; Heuristic programming; Resource allocation; Virtual machine; Cloud servers; Co-evolution; Datacenter; Dynamic resource allocations; Energy aware; Hyper-heuristic algorithms; Optimisations; Resource container; Timetabling; Timetabling and scheduling; Genetic programming","Conference paper","Final","","Scopus","2-s2.0-85206887253"
"Dakić, V.; Kovač, M.; Slovinac, J.","Dakić, Vedran (57213589010); Kovač, Mario (56031685500); Slovinac, Jurica (59217927000)","57213589010; 56031685500; 59217927000","Evolving High-Performance Computing Data Centers with Kubernetes, Performance Analysis, and Dynamic Workload Placement Based on Machine Learning Scheduling","2024","Electronics (Switzerland)","13","13","2651","","","19","10.3390/electronics13132651","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198497070&doi=10.3390%2Felectronics13132651&partnerID=40&md5=5c962a1e8221b3eae77ab7ce0734273c","In the past twenty years, the IT industry has moved away from using physical servers for workload management to workloads consolidated via virtualization and, in the next iteration, further consolidated into containers. Later, container workloads based on Docker and Podman were orchestrated via Kubernetes or OpenShift. On the other hand, high-performance computing (HPC) environments have been lagging in this process, as much work is still needed to figure out how to apply containerization platforms for HPC. Containers have many advantages, as they tend to have less overhead while providing flexibility, modularity, and maintenance benefits. This makes them well-suited for tasks requiring a lot of computing power that are latency- or bandwidth-sensitive. But they are complex to manage, and many daily operations are based on command-line procedures that take years to master. This paper proposes a different architecture based on seamless hardware integration and a user-friendly UI (User Interface). It also offers dynamic workload placement based on real-time performance analysis and prediction and Machine Learning-based scheduling. This solves a prevalent issue in Kubernetes: the suboptimal placement of workloads without needing individual workload schedulers, as they are challenging to write and require much time to debug and test properly. It also enables us to focus on one of the key HPC issues—energy efficiency. Furthermore, the application we developed that implements this architecture helps with the Kubernetes installation process, which is fully automated, no matter which hardware platform we use—x86, ARM, and soon, RISC-V. The results we achieved using this architecture and application are very promising in two areas—the speed of workload scheduling and workload placement on a correct node. This also enables us to focus on one of the key HPC issues—energy efficiency. © 2024 by the authors.","data center architecture; dynamic performance evaluation; hardware and software integration; high-performance computing; Kubernetes","","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85198497070"
"Almeida, G.; Torres do Ó, M.; Francesquini, E.; Cordeiro, D.","Almeida, Guilherme (59152013900); Torres do Ó, Marcelo (57226084964); Francesquini, Emilio (39161187900); Cordeiro, Daniel (36934640400)","59152013900; 57226084964; 39161187900; 36934640400","Reducing carbon emissions of distributed systems: a multi-objective approach","2024","","","","23","","","0","10.1145/3658321.3658364","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194828685&doi=10.1145%2F3658321.3658364&partnerID=40&md5=8592d2d462c5ca76a0be5dab3920c36e","Context The growing usage of platforms for distributed computing and their workloads are requiring more energy to power data centers, and the current consumption is already high. The increasing availability of green energy sources brings opportunities to reduce carbon emissions. Problem It is hard to create software aiming for both performance (makespan) and low brown energy usage. To reduce carbon emissions on distributed platforms, developers need an easy way to program efficiently applications and achieve these objectives. Solution Using OpenMP is an easy way to create and run distributed applications. This paper proposes the use of OpenMP with a new energy-aware scheduling algorithm that aims to minimize brown energy consumption and makespan. IS theory Our multiobjective algorithm (G-MOHEFT) deals with Complexity theory to leverage the Dynamic capabilities of modern distributed platforms. The algorithm implements a heuristic for adapting and redistributing workloads accordingly to different scenarios. Method Containers were used to simulate a distributed OpenMP Cluster (OMPC) platform. Different simulations using previously measured data were used to distribute workloads in different combinations of green energy availability. Summary of Results We study the solution tradeoffs obtained using G-MOHEFT, which varies from saving none to some brown energy consumption by keeping the same or increasing the makespan in exchange. Depending on the scenario, it could even reduce the brown energy consumption to zero. Contributions and impacts to IS Developers can use our algorithm to easily develop distributed software with a reduced carbon footprint using OpenMP in OMPC. © 2024 ACM.","Distributed computing; Green energy; OpenMP; Parallel computing; Tasks scheduling","Application programming interfaces (API); Application programs; Carbon footprint; Distributed computer systems; Green computing; Power management; Scheduling algorithms; Carbon emissions; Distributed platforms; Distributed systems; Energy-consumption; Green energy; Makespan; Multi objective; Openmp; Parallel com- puting; Tasks scheduling; Energy utilization","Conference paper","Final","","Scopus","2-s2.0-85194828685"
"Natarajan, S.; Jacob, J.","Natarajan, Subash (60215218700); Jacob, Jeveen (60215384900)","60215218700; 60215384900","Multi-Cloud Handbook for Developers: Learn how to design and manage cloud-native applications in AWS, Azure, GCP, and more","2024","","","","","1","271","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105023224262&partnerID=40&md5=7c7c5e67ae8a162674dac325a3a6e0a5","Unleash the power of cloud computing with Multi-Cloud Handbook for Developers, your guide to mastering the nuances of cloud-native and multi-cloud, covering practical strategies for design, development, and management. Explore the essential concepts, challenges, and methodologies critical for navigating the complex landscape of modern cloud computing. Using core architectural and design principles (such as microservices and 12-factor architecture) and advanced strategies (such as distributed application design patterns, domain-driven design (DDD), and API-first strategies), you'll learn how to build portable and efficient apps across various cloud platforms. You'll understand how to leverage Infrastructure as Code (IaC), continuous integration and deployment (CI/CD), GitOps, and DevOps practices, along with containerization and orchestration techniques using Docker and Kubernetes. You'll also get to grips with data, security, compliance, and cloud cost management strategies in multi-cloud environments. With real-world case studies, best practices, and insights into future trends, this book will equip you with the skills to develop, manage, troubleshoot, and innovate cloud-native applications across diverse cloud platforms, positioning you at the forefront of the cloud computing revolution. Who is this book for? Ideal for cloud-native and cloud developers, platform engineers, software architects, and IT professionals focused on building and managing cloud-native applications in multi-cloud environments, this book is an indispensable guide for students and researchers seeking insights into cloud-native concepts and multi-cloud architectures. A basic understanding of cloud computing, contemporary software development, system design, and cloud platforms such as AWS, Azure, and GCP, will prove useful. What you will learn • Understand the core structures and implications of cloud-native and multi-cloud apps • Explore key principles and patterns to build agile, scalable, and future-proof apps • Master cloud-native essentials: service mesh, DDD, and API-centric approaches • Implement deployment pipelines with advanced IaC, CI/CD, DevSecOps, and GitOps techniques • Manage and monitor data, security, compliance, and identity access in multi-cloud scenarios • Optimize your cloud costs with shift-left and FinOps practices • Get ready for the future of cloud-native and multi-cloud technology. © 2024 Packt Publishing. All rights reserved.","","Application programming interfaces (API); Application programs; Architectural design; Cloud computing architecture; Cloud platforms; Cloud security; Costs; DevOps; Distributed cloud; Distributed database systems; Engineers; Information management; Software architecture; Software design; Cloud environments; Cloud-computing; Continuous integrations; Design development; Domain-driven designs; Learn+; Multi-clouds; Power; Security compliance; Systems analysis","Book","Final","","Scopus","2-s2.0-105023224262"
"Bouchair, A.","Bouchair, Abderrahim (57208497250)","57208497250","Efficient Virtual Resource Management in Nested Virtualization Environments","2024","","","","","751","756","0","10.1109/3ICT64318.2024.10824265","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217360369&doi=10.1109%2F3ICT64318.2024.10824265&partnerID=40&md5=dadb8fae00f9476e4348c3e3bdba1389","Effective resource management is essential for performance optimization and energy conservation in virtualized cloud systems. Nested virtualization extends traditional virtualization by allowing virtual machines (VMs) to operate inside other VMs, enabling multiple virtualization layers within a single physical server. This paper examines three policy-driven strategies for achieving energy-efficient resource allocation in nested virtualization settings. The first strategy seeks to improve energy efficiency by reducing the number of active physical hosts for VM workloads. The second strategy uses auto-scaling techniques to handle varying workload demands. The third strategy emphasizes allocating resources in a priority manner according to the demands of application performance. This approach is particularly relevant to contemporary Virtual Network Embedding (VNE) because it further increases resource isolation and utilization by incorporating containerization technologies. Using the CloudSim-SDN toolkit, empirical assessments and simulations demonstrate how effectively these policies improve performance and resource efficiency in layered virtualization environments. ©2024 IEEE.","Cloud Computing; Machine Learning; Nested Virtualization; Resource Management; Software-Defined Networking; Virtual Network Embedding","Network embeddings; Cloud-computing; Energy; Machine-learning; Management IS; Nested Virtualization; Resource management; Software-defined networkings; Virtual network embedding; Virtual resources managements; Virtualizations; Virtual environments","Conference paper","Final","","Scopus","2-s2.0-85217360369"
"Araújo, G.; Barbosa, V.; Lima, L.N.; Sabino, A.; Brito, C.; Fe, I.; Rego, P.; Choi, E.; Min, D.; Nguyen, T.A.; Silva, F.A.","Araújo, Gabriel (57221336784); Barbosa, Vandirleya (58697781300); Lima, Luiz Nelson (58481908800); Sabino, Arthur (58481308000); Brito, Carlos (57222736231); Fe, Iure De Sousa (57194271358); Rego, Paulo Antonio Leal (36022938400); Choi, Eunmi (8969714200); Min, Dugki (7201670327); Nguyen, Tuan Anh (56433780500); Silva, Francisco Airton (55817259800)","57221336784; 58697781300; 58481908800; 58481308000; 57222736231; 57194271358; 36022938400; 8969714200; 7201670327; 56433780500; 55817259800","Energy Consumption in Microservices Architectures: A Systematic Literature Review","2024","IEEE Access","12","","","186710","186729","16","10.1109/ACCESS.2024.3389064","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190750888&doi=10.1109%2FACCESS.2024.3389064&partnerID=40&md5=c180332270447a395b7ce6fa19762459","Cloud computing emerges as a paradigm that facilitates on-demand access to technological resources through the mechanism of service virtualization. This virtualization enables the partitioning of hardware resources among applications that are organized into distinct independent modules. The concept of microservice architecture takes advantage of virtualization capabilities to embrace a software architecture strategy focused on the development of applications as assemblies of several interdependent but loosely coupled modules. Nonetheless, the adoption of microservices architecture is accompanied by substantial energy demands to meet the desired standards of performance and availability. Existing research within the domain of microservices has explored various topics pertinent to energy consumption, including elasticity, reliability, performance, and availability. Yet, the diversity of challenges and solutions presents a complex landscape for identifying prevailing research trends and unaddressed gaps in the context of microservices. This study aims to methodically discern, evaluate, and juxtapose the existing research trends and voids concerning energy consumption within microservices. It elucidates a systematic review on the subject of energy consumption in microservices architectures, offering a compilation of references to facilitate more directed future investigations. The initial selection encompassed 3625 articles, which were subsequently narrowed down through three stages of refinement, resulting in 37 articles chosen for an exhaustive review. These selected studies were cataloged and analyzed based on various criteria, including metrics, evaluation methodologies, and architectural typologies, thus uncovering research gaps and emerging trends related to energy consumption in microservice architectures. Furthermore, this inquiry delineates significant research challenges and prospective directions, structured around the key metrics that underpin the reviewed studies: performance, elasticity, scalability, reliability, sustainability, and availability. © 2013 IEEE.","cloud computing; container; energy consumption; Microservices; systematic mapping","Application programs; Computer architecture; Elasticity; Energy utilization; Green computing; Virtual reality; Virtualization; Cloud-computing; Energy-consumption; Microservice; Microservice architecture; Security; Systematic; Systematic mapping; Virtualizations; Cloud computing","Review","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85190750888"
"Symeonides, M.; Tsiopani, N.; Maouris, G.; Trihinas, D.; Pallis, G.; Dikaiakos, M.D.","Symeonides, Moysis (57206469248); Tsiopani, Nicoletta (59830367400); Maouris, Georgios (57215417073); Trihinas, Demetris (56285919400); Pallis, G. (6508027505); Dikaiakos, Marios D. (6701707151)","57206469248; 59830367400; 57215417073; 56285919400; 6508027505; 6701707151","CarbonOracle: Automating Energy Mix & Renewable Energy Source Forecast Modeling for Carbon-Aware Micro Data Centers","2024","","","","","246","255","1","10.1109/UCC63386.2024.00042","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004728120&doi=10.1109%2FUCC63386.2024.00042&partnerID=40&md5=e9f3571bd1af87a94ef609c78eb4eb18","Geo-distributed data centers (DCs) have a substantial impact on global electricity consumption and carbon emissions, with their energy demands expected to increase alongside emerging technologies such as Generative Artificial Intelligence (GenAI) and Natural Language Understanding (NLU). In response to environmental and operational concerns, major cloud providers are investing in DC infrastructures powered by renewable energy sources (RES). However, the design and management of energy-efficient data centers present new challenges. Current forecasting models for RES production and electricity grid energy mix are often limited in accuracy and forecasting horizon, hindering carbon-aware service management. To tackle these challenges, we introduce CarbonOracle, a Machine Learning (ML) service that automates data extraction from self-hosted RES, energy grids, and weather APIs, while also simplifying the ML training and forecast of RES production and electricity grid carbon emissions. Its application programming interface serves ML-based forecasts for RES production (e.g., solar, wind) and energy mix metrics, designed to support carbon-aware deployments, enabling integration with container schedulers and other applications. Through a comprehensive evaluation over a real data center testbed, our results show that CarbonOracle has an error rate of approximately 9% for forecasts related to self-hosted photovoltaic (PV) panels, while its forecasts for electricity grid carbon emissions have an error rate of less than 4%. © 2024 IEEE.","Data centers; Energy Modeling; Machine Learning; Sustainable Computing","Information management; Resource allocation; Software prototyping; Solar fuels; Carbon emissions; Datacenter; Electricity grids; Energy mix; Energy model; Error rate; Machine-learning; Production grids; Renewable energy source; Sustainable computing; Application programs","Conference paper","Final","","Scopus","2-s2.0-105004728120"
"Vercellino, C.; Scionti, A.; Varavallo, G.; Viviani, P.; Vitali, G.; Terzo, O.","Vercellino, Chiara (57766970100); Scionti, Alberto (25927553100); Varavallo, Giuseppe (57217285638); Viviani, Paolo (57194565548); Vitali, Giacomo (57217294605); Terzo, Olivier (27868176400)","57766970100; 25927553100; 57217285638; 57194565548; 57217294605; 27868176400","A Machine Learning Approach for an HPC Use Case: the Jobs Queuing Time Prediction","2023","Future Generation Computer Systems","143","","","215","230","16","10.1016/j.future.2023.01.020","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147843711&doi=10.1016%2Fj.future.2023.01.020&partnerID=40&md5=a4f2921726e470d6964ed7c03893b6a6","High-Performance Computing (HPC) domain provided the necessary tools to support the scientific and industrial advancements we all have seen during the last decades. HPC is a broad domain targeting to provide both software and hardware solutions as well as envisioning methodologies that allow achieving goals of interest, such as system performance and energy efficiency. In this context, supercomputers have been the vehicle for developing and testing the most advanced technologies since their first appearance. Unlike cloud computing resources that are provided to the end-users in an on-demand fashion in the form of virtualized resources (i.e., virtual machines and containers), supercomputers’ resources are generally served through State-of-the-Art batch schedulers (e.g., SLURM, PBS, LSF, HTCondor). As such, the users submit their computational jobs to the system, which manages their execution with the support of queues. In this regard, predicting the behaviour of the jobs in the batch scheduler queues becomes worth it. Indeed, there are many cases where a deeper knowledge of the time experienced by a job in a queue (e.g., the submission of check-pointed jobs or the submission of jobs with execution dependencies) allows exploring more effective workflow orchestration policies. In this work, we focused on applying machine learning (ML) techniques to learn from the historical data collected from the queuing system of real supercomputers, aiming at predicting the time spent on a queue by a given job. Specifically, we applied both unsupervised learning (UL) and supervised learning (SL) techniques to define the most effective features for the prediction task and the actual prediction of the queue waiting time. For this purpose, two approaches have been explored: on one side, the prediction of ranges on jobs’ queuing times (classification approach) and, on the other side, the prediction of the waiting time at the minutes level (regression approach). Experimental results highlight the strong relationship between the SL models’ performances and the way the dataset is split. At the end of the prediction step, we present the uncertainty quantification approach, i.e., a tool to associate the predictions with reliability metrics, based on variance estimation. © 2023","Automatism; Batch scheduler; High performance computing; Machine learning; Queues; Uncertainty quantification","Energy efficiency; Forecasting; Machine learning; Queueing theory; Uncertainty analysis; Automatism; Batch schedulers; Computing domain; High performance computing; Machine learning approaches; Machine-learning; Performance computing; Queue; Time predictions; Uncertainty quantifications; Supercomputers","Article","Final","","Scopus","2-s2.0-85147843711"
"Katal, A.; Dahiya, S.; Choudhury, T.","Katal, Avita (55233742600); Dahiya, Susheela (57211141928); Choudhury, Tanupriya (57193140084)","55233742600; 57211141928; 57193140084","Energy efficiency in cloud computing data centers: a survey on software technologies","2023","Cluster Computing","26","3","","1845","1875","329","10.1007/s10586-022-03713-0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137223667&doi=10.1007%2Fs10586-022-03713-0&partnerID=40&md5=e59f12ec54b09fc41c0ebfec7d42e726","Cloud computing is a commercial and economic paradigm that has gained traction since 2006 and is presently the most significant technology in IT sector. From the notion of cloud computing to its energy efficiency, cloud has been the subject of much discussion. The energy consumption of data centres alone will rise from 200 TWh in 2016 to 2967 TWh in 2030. The data centres require a lot of power to provide services, which increases CO2 emissions. In this survey paper, software-based technologies that can be used for building green data centers and include power management at individual software level has been discussed. The paper discusses the energy efficiency in containers and problem-solving approaches used for reducing power consumption in data centers. Further, the paper also gives details about the impact of data centers on environment that includes the e-waste and the various standards opted by different countries for giving rating to the data centers. This article goes beyond just demonstrating new green cloud computing possibilities. Instead, it focuses the attention and resources of academia and society on a critical issue: long-term technological advancement. The article covers the new technologies that can be applied at the individual software level that includes techniques applied at virtualization level, operating system level and application level. It clearly defines different measures at each level to reduce the energy consumption that clearly adds value to the current environmental problem of pollution reduction. This article also addresses the difficulties, concerns, and needs that cloud data centres and cloud organisations must grasp, as well as some of the factors and case studies that influence green cloud usage. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Cloud Computing; Containerization; Data center; Load balancing; Workload categorization","Application programs; Containers; Electronic Waste; Energy efficiency; Energy utilization; Green computing; Surveys; Cloud-computing; CO2 emissions; Containerization; Datacenter; Energy-consumption; Green Clouds; Load-Balancing; Power; Software technology; Workload categorization; Cloud computing","Article","Final","All Open Access; Green Accepted Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85137223667"
"Moocheet, N.; Jaumard, B.; Thibault, P.; Eleftheriadis, L.","Moocheet, Nalveer (58838217200); Jaumard, Brigitte (56847096800); Thibault, Pierre (58838971700); Eleftheriadis, Lackis (57837398100)","58838217200; 56847096800; 58838971700; 57837398100","A Sensor Predictive Model for Power Consumption using Machine Learning","2023","","","","","238","246","2","10.1109/CloudNet59005.2023.10490084","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183024045&doi=10.1109%2FCloudNet59005.2023.10490084&partnerID=40&md5=56548ec979c5c0892f158a35e05515a9","Reducing the power consumption of computing devices remains a challenge for the data center industry. In 2022, it represents approximately 2% of global electricity consumption and 1% of global greenhouse gas emissions. In addition, data centers must integrate the 5G and B5G challenges into their strategies, by increasing the computing resources available to face higher-quality service constraints. Indeed, 5G and B5G future networks are increasingly software-oriented and therefore, rely heavily on cloud computing to process large amounts of data from multiple sources in real-time.Several research works on energy management have been proposed to ensure a reduction of the energy consumed by the various components of a data center (e.g., software, computing devices, or cooling systems). However, to optimize the energy consumption of computing devices (e.g., virtual machines/container operations), it is essential to have an accurate model for predicting power consumption. Thus, we propose in this study a new sensor predictive model to predict the dynamic power consumption of cloud computing devices with high accuracy.Our proposal takes advantage of the various sensors that are now embedded in physical machines, or more generally in cloud server machines, as well as Performance Monitoring Counters to implement a Machine Learning power prediction model.The performance evaluation results confirm that our power consumption prediction models outperform previous literature models in terms of accuracy. Indeed, our best model achieves a R2 score of 93.6% which is higher than the compared baseline model by 21.1%. © 2023 IEEE.","Machine Learning; Performance Monitoring Counter; Power Consumption; Prediction; Sensor","5G mobile communication systems; Cloud computing; Computing power; Cooling systems; Forecasting; Gas emissions; Green computing; Greenhouse gases; Machine learning; Cloud-computing; Computing devices; Datacenter; Electricity-consumption; Greenhouse gas emissions; Machine-learning; Performance monitoring counter; Performance-monitoring; Prediction modelling; Predictive models; Electric power utilization","Conference paper","Final","","Scopus","2-s2.0-85183024045"
"Fe, I.; Nguyen, T.A.; Soares, A.B.; Son, S.; Choi, E.; Min, D.; Lee, J.-W.; Silva, F.A.","Fe, Iure De Sousa (57194271358); Nguyen, Tuan Anh (56433780500); Soares, Andrec B. (58784995600); Son, Seokho (55436728000); Choi, Eunmi (8969714200); Min, Dugki (7201670327); Lee, Jaewoo (57203409283); Silva, Francisco Airton (55817259800)","57194271358; 56433780500; 58784995600; 55436728000; 8969714200; 7201670327; 57203409283; 55817259800","Model-Driven Dependability and Power Consumption Quantification of Kubernetes-Based Cloud-Fog Continuum","2023","IEEE Access","11","","","140826","140852","3","10.1109/ACCESS.2023.3340195","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179793315&doi=10.1109%2FACCESS.2023.3340195&partnerID=40&md5=d8dd33abcbadcaebdf6daaab812a0221","System dependability is pivotal for the reliable execution of designated computing functions. With the emergence of cloud-fog computing and microservices architectures, new challenges and opportunities arise in evaluating system dependability. Enhancing dependability in microservices often involves component replication, potentially increasing energy costs. Thus, discerning optimal redundancy strategies and understanding their energy implications is crucial for both cost efficiency and ecological sustainability. This paper presents a model-driven approach to evaluate the dependability and energy consumption of cloud-fog systems, utilizing Kubernetes, a container application orchestration platform. The developed model considers various determinants affecting system dependability, including hardware and software reliability, resource accessibility, and support personnel availability. Empirical studies validate the model's effectiveness, demonstrating a 22.33% increase in system availability with only a 1.33% rise in energy consumption. Moreover, this methodology provides a structured framework for understanding cloud-fog system dependability, serves as a reference for comparing dependability across different systems, and aids in resource allocation optimization. This research significantly contributes to the efforts to enhance cloud-fog system dependability. © 2013 IEEE.","Cloud-fog continuum; dependability; Kubernetes; stochastic modeling","Computing power; Edge computing; Electric power utilization; Fog; Fog computing; Green computing; Software reliability; Stochastic models; Stochastic systems; Cloud-computing; Cloud-fog continuum; Dependability; Energy-consumption; Kubernetes; Modeling; Power demands; Stochastic-modeling; System dependability; Random processes","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85179793315"
"","","","22nd International Scientific and Practical Conference Information and Communication Technologies and Sustainable Development, ICT&SD 2022","2023","Lecture Notes in Networks and Systems","809 LNNS","","","","","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177815281&partnerID=40&md5=8a8557218df883ba180e1ae8c83b9efd","The proceedings contain 26 papers. The special focus in this conference is on Information and Communication Technologies and Sustainable Development. The topics include: Network Monitoring Index in the Information Security Management System of Critical Information Infrastructure Objects; the Chézy Roughness Coefficient Computing Using an Artificial Neural Network to Support the Mathematical Modelling of River Flows; genomic Data Machined: The Random Forest Algorithm for Discovering Breast Cancer Biomarkers; features of Message Transport Service in Automated Special-Purpose Systems; Impact of LoRaWAN Operational Parameters on Energy Efficiency and Ways to Improve It; enhancing Application Layer Multicast Using Multi-objective Optimization Techniques; a New Approach to Interpolation and Approximation of Boundary Trajectories of Electron Beams for Realizing Cloud Computing Using Root-Polynomial Functions; cloud-Based Technologies Google Earth Engine for Monitoring Surface Deformation of the Solotvyno Agglomeration; enhancing Resource Availability: Indicators and Strategies for Optimizing the Kubernetes Network; study of Energy Efficient Technologies for Workload Processing in Data Centers; IT Platform for the Formation of Digital Duplicates for Museum Exhibits; notation System for Comparing and Synthesis of Intelligent Key Phrase Extraction Methods for Ontological Models in Information Systems; sustainable Development in the Global Information Space; a Research Method of Software-Defined Networks Asymptotic Properties with Markov Intrusion of Randomness; cloud Platforms and Technologies for Big Satellite Data Processing; method Radio Resource Allocation in Cognitive Radio Network; mathematical Model for Evaluation of Interference Dispersion for 5G Mobile Communication Systems; selecting a Polynomial for Estimating the Motion Parameters of a Permanently Maneuvering Group of Unmanned Aerial Vehicles; preface.","","","Conference review","Final","","Scopus","2-s2.0-85177815281"
"Shivam; Kumar, D.","Shivam (59730536100); Kumar, Dinesh (57202810166)","59730536100; 57202810166","Containerized Deployment of Microservices in Cloud Computing","2023","","","","","35","59","1","10.1201/9781003438588-3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174770374&doi=10.1201%2F9781003438588-3&partnerID=40&md5=b66865a66ccec057778b0251c9ccbf4e","Containerization results in the proliferation of several benefits to cloud computing due to its speed, scalability, and security features. Additionally, the growing popularity of containerizing sophisticated applications (such as Netflix, Spotify, and the Times of India) and deploying them over the cloud has aided businesses in the development, testing, management, deployment, and security of their software and applications. This increase in popularity, embarked demand for research in the areas of efficiency and optimization of metrics such as resources (CPU, memory, storage, and network), power consumption, and cost, to make the deployment more feasible and efficient in developing and scaling services of the applications. This work proposed heuristics and PSO-based approach to optimize microservices-to-container placement and container-to-server placement, respectively. The proposed methods efficiently utilize resources and power, thus providing cost efficiency to the deployment and auto-scaling of the microservices of an application. More specifically, dynamic bin packing and Particle-swarm optimization techniques are adopted as optimization algorithms for minimizing resource wastage and power consumption. The proposed algorithms are then compared with state-of-the-art and the results verify the effectiveness of the proposed work. © 2024 selection and editorial matter, Mohammad Sajid, Anil Kumar Sagar, Jagendra Singh, Osamah Ibrahim Khalaf and Mukesh Prasad; individual chapters, the contributors.","","","Book chapter","Final","","Scopus","2-s2.0-85174770374"
"Helali, L.; Omri, M.N.","Helali, Leila (57191430358); Omri, Mohamed Nazih (6604032258)","57191430358; 6604032258","Intelligent and compliant dynamic software license consolidation in cloud environment","2022","Computing","104","12","","2749","2783","2","10.1007/s00607-022-01106-0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134723471&doi=10.1007%2Fs00607-022-01106-0&partnerID=40&md5=33f8a26c4e09808ec30eafabac147092","Based on the virtualization technology and pushed by the softwarization paradigm and the actual demand for services and resources, commercial cloud data centers know an unprecedented expansion. The systematic presence of software and the services generated enabled the development of the already dense application expenses. This causes, not only a cost explosion, especially when proprietary solutions protected by licenses are in hand, but also, represents a critical need in terms of software asset and resource management at the SaaS level. In addition to these costs, inefficient resource utilization, and the resulting energy represent an important part of the operational expenditure of data centers and are still a hot topic despite the consolidation initiatives put in place. The main objective of the consolidation service is to maximize resource exploitation while minimizing energy consumption and costs, among others. Even so, we have noticed that the reported literature doesn’t treat license management in the cloud environment as a whole, especially, from the resource management perspective and the overwhelming majority of the consolidation work focuses on resource optimization at the IaaS level. Therefore, we propose a reinforcement learning-based scheme that allows efficient use of resources and optimizes costs, energy consumption, and resource wastage, while remaining compliant. The experimental results show that our intelligent consolidator outperforms the baseline approaches according to the evaluation metrics used regardless of the resource heterogeneity and the data center dimensionality. © 2022, The Author(s), under exclusive licence to Springer-Verlag GmbH Austria, part of Springer Nature.","Artificial intelligence; Cloud data centers; Compliance; Containerization; Energy-efficiency; Reinforcement Learning; Resource management","Application programs; Energy efficiency; Energy utilization; Environmental management; Green computing; Information management; Learning systems; Natural resources management; Resource allocation; Cloud data centers; Cloud environments; Compliance; Containerization; Datacenter; Dynamic softwares; Reinforcement learnings; Resource management; Software license; Virtualization technologies; Reinforcement learning","Article","Final","All Open Access; Bronze Open Access; Green Accepted Open Access; Green Open Access","Scopus","2-s2.0-85134723471"
"Helali, L.; Omri, M.N.","Helali, Leila (57191430358); Omri, Mohamed Nazih (6604032258)","57191430358; 6604032258","Software License Consolidation and Resource Optimization in Container-based Virtualized Data Centers","2022","Journal of Grid Computing","20","2","13","","","9","10.1007/s10723-022-09602-5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128605954&doi=10.1007%2Fs10723-022-09602-5&partnerID=40&md5=d567a46724ae55665947f26c67f2451c","Over the past two years, the pandemic-induced demand for resources and services has accelerated. The paradigm of softwarization as well as webization, services, and network virtualization have caused disruptive changes within cloudified environments. Software services are the most crucial part of the evolution of these environments. So far, proprietary solutions are particularly emphasized owing to the current economic climate, such as the numerous cloud services that depend on commercial software. Thus, the ubiquity of this kind of software in the cloud creates a new dimension for software license (SL) optimization. For a prosperous company, optimal use of SL resources remains a capital and essential criterion. Managing these resources, virtualized hardware resources, energy, and costs generated in the cloud is a recent challenge. Although many consolidation strategies have been developed in the area of resource management to optimize standard resources and the resulting energy consumption, this problem remains topical despite optimization efforts. Indeed, appropriate schemes and models for the dynamic consolidation of resources and the optimization of data center states have always been under constant investigation and extensive study and development at the infrastructure level. But, so far, the software service level lacks models and algorithms that offer dynamic optimization of resource allocation and workload consolidation, especially when proprietary solutions are in hand. In this article, we offer suitable heuristics for dynamic software license consolidation (DSLC) and optimization of resources. These tend to reduce costs within a cloud data center and minimize overall power consumption. Our solutions allow us to optimize the monetary costs of the users and offer more flexibility to reduce the infrastructure costs. The experimental studies carried out from small to relatively big data centers, in both homogeneous and heterogeneous resource scenarios, show the efficiency of our solutions. Thus, we recorded a rate of 71.4% energy savings, 79.91% total cost savings, 80.25% license cost, and 76.5% virtual machine (VM) cost. We also highlighted a rate of 78% of deactivated physical machine (PMs) and 76.5% of released VMs. © 2022, The Author(s), under exclusive licence to Springer Nature B.V.","Cloud computing; Commercial software; Consolidation; Containerization; Energy efficiency; License optimization; Resource management; Virtualization","Cloud computing; Containers; Copyrights; Cost reduction; Economics; Energy efficiency; Energy utilization; Green computing; Information management; Natural resources management; Optimization; Virtual machine; Virtual reality; Virtualization; Cloud-computing; Commercial software; Datacenter; License optimization; Optimisations; Proprietary solutions; Resource management; Software license; Software services; Virtualizations; Resource allocation","Article","Final","","Scopus","2-s2.0-85128605954"
"Janbi, N.; Mehmood, R.; Katib, I.; Albeshri, A.; Corchado Rodríguez, J.M.; Yigitcanlar, T.","Janbi, Nourah Fahad (57201743503); Mehmood, Rashid Ibrahim (25643246000); Katib, Iyad A. (26534538800); Albeshri, Aiiad Ahmad (36617092600); Corchado Rodríguez, Juan M. (7006360842); Yigitcanlar, Tan (6505536041)","57201743503; 25643246000; 26534538800; 36617092600; 7006360842; 6505536041","Imtidad: A Reference Architecture and a Case Study on Developing Distributed AI Services for Skin Disease Diagnosis over Cloud, Fog and Edge","2022","Sensors","22","5","1854","","","40","10.3390/s22051854","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126324127&doi=10.3390%2Fs22051854&partnerID=40&md5=ea7a99e2317b58f6c53dbecd9fde6e9c","Several factors are motivating the development of preventive, personalized, connected, virtual, and ubiquitous healthcare services. These factors include declining public health, increase in chronic diseases, an ageing population, rising healthcare costs, the need to bring intelligence near the user for privacy, security, performance, and costs reasons, as well as COVID-19. Motivated by these drivers, this paper proposes, implements, and evaluates a reference architecture called Imtidad that provides Distributed Artificial Intelligence (AI) as a Service (DAIaaS) over cloud, fog, and edge using a service catalog case study containing 22 AI skin disease diagnosis services. These services belong to four service classes that are distinguished based on software platforms (containerized gRPC, gRPC, Android, and Android Nearby) and are executed on a range of hardware platforms (Google Cloud, HP Pavilion Laptop, NVIDIA Jetson nano, Raspberry Pi Model B, Samsung Galaxy S9, and Samsung Galaxy Note 4) and four network types (Fiber, Cellular, Wi-Fi, and Bluetooth). The AI models for the diagnosis include two standard Deep Neural Networks and two Tiny AI deep models to enable their execution at the edge, trained and tested using 10,015 real-life dermatoscopic images. The services are evaluated using several benchmarks including model service value, response time, energy consumption, and network transfer time. A DL service on a local smartphone provides the best service in terms of both energy and speed, followed by a Raspberry Pi edge device and a laptop in fog. The services are designed to enable different use cases, such as patient diagnosis at home or sending diagnosis requests to travelling medical professionals through a fog device or cloud. This is the pioneering work that provides a reference architecture and such a detailed implementation and treatment of DAIaaS services, and is also expected to have an extensive impact on developing smart distributed service infrastructures for healthcare and other sectors. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Cloud computing; Distributed AI as a service (DAIaaS); Edge computing; Fog computing; Healthcare; Reference architecture; Skin disease diagnosis; Smart cities; Smart healthcare; Smart societies; TensorFlow; Tiny AI; Tiny ML","Computer architecture; Deep neural networks; Drones; Edge computing; Energy utilization; Fog; mHealth; Mobile security; Network architecture; Smart city; Smartphones; Wi-Fi; Cloud-computing; Disease diagnosis; Distributed Artificial Intelligence; Distributed artificial intelligence as a service; Reference architecture; Skin disease; Skin disease diagnose; Smart healthcare; Smart society; Tensorflow; Tiny artificial intelligence; Tiny ML; Fog computing; artificial intelligence; human; skin disease; software; Artificial Intelligence; COVID-19; Humans; SARS-CoV-2; Skin Diseases; Software","Article","Final","All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access","Scopus","2-s2.0-85126324127"
"Alhindi, A.; Djemame, K.; Banaie, F.B.","Alhindi, Abdulaziz (58153549700); Djemame, Karim (6601958964); Banaie, Fatemeh (55655107300)","58153549700; 6601958964; 55655107300","On the Power Consumption of Serverless Functions: An Evaluation of OpenFaaS","2022","","","","","366","371","13","10.1109/UCC56403.2022.00064","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150679165&doi=10.1109%2FUCC56403.2022.00064&partnerID=40&md5=f601604ceae2ebefa1dd6a5bf21e9207","The rapid growth in cloud-based technologies has introduced the need for very large data centres to meet the increasing demand for cloud services. One of the main challenges in managing these data centres is the sharp increase of power consumption. Research has therefore tackled the issue of power/energy efficiency in cloud data centres. Serverless computing is a cloud computing execution model that gives software developers the option to deploy their code without the need to configure servers, operating systems or runtime libraries, thus allowing them to invest less effort and capital in infrastructure management. This paper investigates whether serverless computing has the ability to support power efficiency. To this aim, a number of experiments are conducted to compare the power consumption of a serverless platform, OpenFaaS, against Docker containers with the consideration of applications and benchmarks. The experimental results show that OpenFaaS is more power efficient than Docker when the processor and memory are under stress. © 2022 IEEE.","Container; Docker; OpenFaaS; Power Consumption; Power Efficiency; Serverless","Benchmarking; Containers; Energy efficiency; Green computing; Cloud services; Cloud-based; Datacenter; Docker; Openfaas; Power-efficiency; Rapid growth; Serverless; Sharp increase; Very large datum; Electric power utilization","Conference paper","Final","","Scopus","2-s2.0-85150679165"
"Touloupou, M.; Kapassa, E.; Rizou, S.","Touloupou, Marios (57202604831); Kapassa, Evgenia (57201027060); Rizou, Stamatia (34977594700)","57202604831; 57201027060; 34977594700","Cloud Orchestration for Optimized Computing Efficiency: The Case of Wind Resource Modelling","2022","","","","","79","84","1","10.1109/CloudNet55617.2022.9978809","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146109929&doi=10.1109%2FCloudNet55617.2022.9978809&partnerID=40&md5=5c81700e3b3d0d7478dd67af08d699dc","The Weather Research and Forecasting (WRF) model is a multi-purpose open source weather model, which is widely used by the wind energy community. However, the state of the art applications that use the WRF model for wind resources do not always meet the sector's needs. The fundamental challenge is the massive quantity of data required for accurate modelling, as well as the expenses associated with the computing of such massive volumes of data, implying that state-of-the-art approaches are not appropriate. Our work tackles this issue with the use of a novel orchestration framework relying on cloud computing infrastructure. Cloud computing platforms provide robust infrastructure that allows for the deployment of production-level services as well as extended processing and storage capacity. This paper presents the overall software architecture of a Cloud Orchestration System, enabling the dynamic and flexible cloud deployment of containerized wind resource model chains. Moreover, a set of performance, functional and operational criteria are presented and evaluated, using a real use case of a wind resource model. © 2022 IEEE.","cloud; computing efficiency; optimal deployment; orchestration; wind resources","Cloud computing; Digital storage; Open source software; Weather forecasting; Computing efficiency; Multi-purpose; Open-source; Optimal deployment; Orchestration; Resource modelling; State of the art; Weather modeling; Weather research and forecasting models; Wind resources; Wind power","Conference paper","Final","All Open Access; Green Accepted Open Access; Green Open Access","Scopus","2-s2.0-85146109929"
"Saboor, A.; Mahmood, A.K.; Omar, A.H.; Hassan, M.F.; Shah, S.N.M.; Ahmadian, A.","Saboor, Abdul (23986039400); Mahmood, Ahmad Kamil (24344688000); Omar, Abdullah Hisam (56446319700); Hassan, Mohd Fadzil (55372860500); Shah, Syed Nasir Mehmood (57213233304); Ahmadian, Ali (59760609700)","23986039400; 24344688000; 56446319700; 55372860500; 57213233304; 59760609700","Enabling rank-based distribution of microservices among containers for green cloud computing environment","2022","Peer-to-Peer Networking and Applications","15","1","","77","91","21","10.1007/s12083-021-01218-y","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112841408&doi=10.1007%2Fs12083-021-01218-y&partnerID=40&md5=68fc8139a7dfb8ccf92ff4ee0b97cf69","Microservices architecture is a functional software design methodology that promises the redefinition of the architectural style that aims to create a single application as a suite of tiny, loosely coupled services or components, each performing its own tasks and interacting with each other. The cloud services widely shifted from monoliths to microservices and gained the popularity for use in scalable cloud application. The usage of microservices involved intensive network communication to call number of interdependent microservices running inside the cloud nodes. It provides flexibility in the delivery of service but also increases energy usage and poor service efficiency which results in increased carbon emissions. To solve these issues, the prevailing technologies were designed for single unit monolithic cloud applications, and not tailored for the chain oriented service delivery. This study addresses the dynamic provisioning of containers and respective microservices in cloud computing environment by building rank-based profiles and using those profiles for allocation of web application’s microservices along with containers to the cloud data centers. The MicroRanker service is proposed to rank all of the participating microservices and distribute them across different nodes even before the execution of the cloud services. Further, the MicroRanker service is utilized to dynamically update the container placement due to continuous DevOps actions. The proposed solution was tested using custom built simulation environment. The achieved results showed that the distribution of containers along with respective microservices in accordance with MicroRanker service resulted in less energy consumption (i.e. between 81.6 kWh-87.7 kWh compared to 88.9 kWh-95.7 kWh) and significantly lowered the emission of carbon (i.e. between 5.92 kg-33.31 kg compared to 17.2 kg-47.35 kg) due to higher utilization of renewable energy. The use of rank-based microservices distribution also decreased response time (i.e. between 29 ms-142 ms compared to 106 ms-217 ms) due to the availability of the container along with microservice within the same data center region. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Cloud computing; Containers; High performance computing; Microservices; Optimization; Ranking","Application programs; Carbon; Cloud computing; Containers; Energy utilization; Software design; Web services; Architectural style; Cloud computing environments; Cloud data centers; Dynamic provisioning; Network communications; Renewable energies; Simulation environment; Software design methodologies; Green computing","Article","Final","","Scopus","2-s2.0-85112841408"
"Hanussek, M.; Bartusch, F.; Ger, J.K.","Hanussek, Maximilian (57208688350); Bartusch, Felix (57208688180); Ger, Jens Kru (57226338974)","57208688350; 57208688180; 57226338974","Performance and scaling behavior of bioinformatic applications in virtualization environments to create awareness for the efficient use of compute resources","2021","PLOS Computational Biology","17","7","e1009244","","","10","10.1371/journal.pcbi.1009244","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111226775&doi=10.1371%2Fjournal.pcbi.1009244&partnerID=40&md5=1bd61bfb401e7d8804efd4b44976fdf0","The large amount of biological data available in the current times, makes it necessary to use tools and applications based on sophisticated and efficient algorithms, developed in the area of bioinformatics. Further, access to high performance computing resources is necessary, to achieve results in reasonable time. To speed up applications and utilize available compute resources as efficient as possible, software developers make use of parallelization mechanisms, like multithreading. Many of the available tools in bioinformatics offer multithreading capabilities, but more compute power is not always helpful. In this study we investigated the behavior of well-known applications in bioinformatics, regarding their performance in the terms of scaling, different virtual environments and different datasets with our benchmarking tool suite BOOTABLE. The tool suite includes the tools BBMap, Bowtie2, BWA, Velvet, IDBA, SPAdes, Clustal Omega, MAFFT, SINA and GROMACS. In addition we added an application using the machine learning framework TensorFlow. Machine learning is not directly part of bioinformatics but applied to many biological problems, especially in the context of medical images (X-ray photographs). The mentioned tools have been analyzed in two different virtual environments, a virtual machine environment based on the OpenStack cloud software and in a Docker environment. The gained performance values were compared to a bare-metal setup and among each other. The study reveals, that the used virtual environments produce an overhead in the range of seven to twenty-five percent compared to the bare-metal environment. The scaling measurements showed, that some of the analyzed tools do not benefit from using larger amounts of computing resources, whereas others showed an almost linear scaling behavior. The findings of this study have been generalized as far as possible and should help users to find the best amount of resources for their analysis. Further, the results provide valuable information for resource providers to handle their resources as efficiently as possible and raise the user community's awareness of the efficient usage of computing resources. Copyright: © 2021 Hanussek et al.","","Application programs; Benchmarking; Bioinformatics; Machine learning; Medical imaging; Network security; Virtual machine; Virtualization; Bare metals; Bioinformatics applications; Compute resources; Computing resource; Large amounts; Machine-learning; Performance; Scaling behaviours; Scalings; Toolsuite; Virtual reality; article; awareness; benchmarking; bioinformatics; human; machine learning; photography; shipyard worker; software; X ray; algorithm; biology; cloud computing; computer; computer analysis; computer assisted diagnosis; computer interface; factual database; high throughput sequencing; procedures; sequence alignment; statistical analysis; Algorithms; Cloud Computing; Computational Biology; Computers; Computing Methodologies; Data Interpretation, Statistical; Databases, Factual; High-Throughput Nucleotide Sequencing; Humans; Image Interpretation, Computer-Assisted; Machine Learning; Sequence Alignment; Software; User-Computer Interface","Article","Final","All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access","Scopus","2-s2.0-85111226775"
"Nasrin, S.; Sahryer, T.I.M.F.; Islam, A.B.M.A.; Noor, J.","Nasrin, Sabiha (57460901400); Sahryer, T. I.M.Fahim (57461753700); Islam, Arnob B.M.Alim Al (57203124719); Noor, Jannatun (57193917145)","57460901400; 57461753700; 57203124719; 57193917145","Feature and Performance Based Comparative Study on Serverless Frameworks","2021","","","","","","","2","10.1109/ICCIT54785.2021.9689779","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125017909&doi=10.1109%2FICCIT54785.2021.9689779&partnerID=40&md5=054cf3f0c6307cf011efed82b6a6f151","We conduct experiments on the different public clouds provided in this paper to bring out a comparative study, to help the developers understand the diversity of the platforms. This study will help them choose a suitable platform for their desired application. The contemporary and future usage of this diverse nature of cloud computing is hugely demanding. Thus, we elaborate our study on serverless cloud computing to suffice the demand. Serverless prevents a great deal of unessential consumption of power and is a pay-as-you-go service. This technology has added a great impact on software and application development. Although the major obstacle to this development field is that there is not enough documentation on how the big companies provide this facility and how their architecture is built. The comparative study on this diverse platform is missing in the literature. Therefore, our research is based on the on-demand serverless use cases and comparative study with necessary measures. This can be effective and efficient to use for further serverless implementation. Hence, we and others can follow our research for understanding the technical complexity. © 2021 IEEE.","AWS; Azure; features; Fission; GCP; Kubernetes; Serverless","Application programs; AWS; Azure; Cloud-computing; Comparatives studies; Feature; Feature-based; Fission; GCP; Kubernetes; Serverless; Cloud computing","Conference paper","Final","","Scopus","2-s2.0-85125017909"
"Ara, G.; Lai, L.; Cucinotta, T.; Abeni, L.; Vitucci, C.","Ara, Gabriele (57212493882); Lai, Leonardo (57216224464); Cucinotta, Tommaso (6506625665); Abeni, Luca (55922066400); Vitucci, Carlo (57194946547)","57212493882; 57216224464; 6506625665; 55922066400; 57194946547","A Framework for Comparative Evaluation of High-Performance Virtualized Networking Mechanisms","2021","Communications in Computer and Information Science","1399 CCIS","","","59","83","5","10.1007/978-3-030-72369-9_3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104764763&doi=10.1007%2F978-3-030-72369-9_3&partnerID=40&md5=596cff4ef80e3bc2c03828d3dfddbad3","This paper presents an extension to a software framework designed to evaluate the efficiency of different software and hardware-accelerated virtual switches, each commonly adopted on Linux to provide virtual network connectivity to containers in high-performance scenarios, like in Network Function Virtualization (NFV). We present results from the use of our tools, showing the performance of multiple high-performance networking frameworks on a specific platform, comparing the collected data for various key metrics, namely throughput, latency and scalability, with respect to the required computational power. © 2021, Springer Nature Switzerland AG.","Cloud computing; Containers; DPDK; Kernel bypass; Netmap; NFV","Cloud computing; Computer operating systems; Computer programming; Comparative evaluations; Computational power; High-performance networking; In networks; Networking mechanisms; Software and hardwares; Software frameworks; Virtual networks; Network function virtualization","Conference paper","Final","All Open Access; Green Accepted Open Access; Green Open Access","Scopus","2-s2.0-85104764763"
"Bystrov, O.; Kačeniauskas, A.; Pacevič, R.; Starikovičius, V.; Maknickas, A.; Stupak, E.; Igumenov, A.","Bystrov, Oleg V. (57221814060); Kačeniauskas, Arnas (35219861700); Pacevič, Ruslan (36114828100); Starikovičius, Vadimas (55918546500); Maknickas, Algirdas Antano (14825473400); Stupak, Eugeniuš (8397192700); Igumenov, Aleksandr (36106706900)","57221814060; 35219861700; 36114828100; 55918546500; 14825473400; 8397192700; 36106706900","Performance evaluation of parallel haemodynamic computations on heterogeneous clouds","2021","Computing and Informatics","39","4","","695","723","7","10.31577/CAI_2020_4_695","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100300256&doi=10.31577%2FCAI_2020_4_695&partnerID=40&md5=71bc1402f8b68a5e5a5c5de51749ae37","The article presents performance evaluation of parallel haemodynamic flow computations on heterogeneous resources of the OpenStack cloud infrastructure. The main focus is on the parallel performance analysis, energy consumption and virtualization overhead of the developed software service based on ANSYS Fluent platform which runs on Docker containers of the private university cloud. The haemodynamic aortic valve flow described by incompressible Navier-Stokes equations is considered as a target application of the hosted cloud infrastructure. The parallel performance of the developed software service is assessed measuring the parallel speedup of computations carried out on virtualized heterogeneous resources. The performance measured on Docker containers is compared with that obtained by using the native hardware. The alternative solution algorithms are explored in terms of the parallel performance and power consumption. The investigation of a trade-off between the computing speed and the consumed energy is performed by using Pareto front analysis and a linear scalarization method. © 2020 Slovak Academy of Sciences. All rights reserved.","Bi-objective optimization problem; Cloud computing; Energy consumption; Haemodynamic flows; Parallel computing; Parallel performance analysis","Containers; Economic and social effects; Navier Stokes equations; Parallel flow; Bi-objective optimisation problems; Cloud infrastructures; Cloud-computing; Energy-consumption; Haemodynamics; Hemodynamic flow; Heterogeneous resources; Parallel com- puting; Parallel performance analysis; Performances evaluation; Energy utilization","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85100300256"
"Ranjan, R.; Thakur, I.S.; Aujla, G.S.; Kumar, N.; Zomaya, A.Y.","Ranjan, Rohit (57219315202); Thakur, Ishan Singh (57219319898); Aujla, Gagangeet Singh (56218282200); Kumar, Neerai Sathish (57206866080); Zomaya, Albert Y.H. (7005128430)","57219315202; 57219319898; 56218282200; 57206866080; 7005128430","Energy-Efficient Workflow Scheduling Using Container-Based Virtualization in Software-Defined Data Centers","2020","IEEE Transactions on Industrial Informatics","16","12","9057431","7646","7657","34","10.1109/TII.2020.2985030","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092197336&doi=10.1109%2FTII.2020.2985030&partnerID=40&md5=f64f77d10480410596d7e1612952c756","Workflow scheduling is one of the most difficult tasks due to the variation in the traffic flows generated from diverse cloud applications. Hence, in this article, a container-based virtualization is used to design an energy-efficient workflow scheduling in software-defined data centers. The containers provide the flexibility to the applications to access the underlying resource as per their requirements. Moreover, a runtime scheduler is responsible to handle all the scheduling decisions in the proposed workflow scheduling scheme. Even more, a doubly linked list-based access mechanism is used to provide access to the servers and virtual machines by traversing both ways. Finally, a hashing scheme is used to select an ideal location for the allocation of the containers. The proposed scheme is evaluated with respect to different performance metrics (makespan, execution time, fault tolerance, energy consumption, etc.) on the real data traces. The results obtained depict the superiority of the proposed scheme in comparison to the other existing schemes of its category. © 2005-2012 IEEE.","Cloud computing; containers; software-defined data centers (SDDCs); virtual machine; workflow scheduling","Containers; Energy efficiency; Energy utilization; Fault tolerance; Virtualization; Access mechanism; Cloud applications; Data centers; Energy efficient; Performance metrics; Scheduling decisions; Traffic flow; Workflow scheduling; Scheduling","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85092197336"
"Xu, C.; Zheng, Y.","Xu, Changlin (57746347500); Zheng, Yan (57194279762)","57746347500; 57194279762","Cloud-Edge Monitoring System Framework for Plug-In Hybrid Electric Vehicles","2020","","","","9292880","369","374","0","10.1109/HVDC50696.2020.9292880","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099433450&doi=10.1109%2FHVDC50696.2020.9292880&partnerID=40&md5=52a6bf804606a41293b9b07b393a2fd5","Plug-in hybrid electric vehicles (PHEVs) are widely used in the field of new energy vehicles due to their improvement of electric power efficiency and low emission. This paper proposes a new cloud-edge monitoring system (CEMS) architecture for PHEV, which includes a container as a service (CaaS) cloud platform and edge nodes to realize PHEVs status data storage and analysis. In CaaS, open-source software featured in the proposed platform enables the user to implement data processing. Then a container scheduling algorithm is proposed to optimize data querying speed. Finally, simulation results illustrate the advantages of the proposed architecture. © 2020 IEEE.","cloud computing; container scheduling; edge computing; PHEV","Containers; Data handling; Digital storage; HVDC power transmission; Monitoring; Open source software; Open systems; Platform as a Service (PaaS); Storage as a service (STaaS); Cloud platforms; Container scheduling; Data querying; Electric power efficiency; Monitoring system; New energy vehicles; Plug-in hybrid electric vehicles; Proposed architectures; Plug-in hybrid vehicles","Conference paper","Final","","Scopus","2-s2.0-85099433450"
"Daraghmeh, N.; Arianyan, E.; Buyya, R.","Daraghmeh, Mustafa (57217147405); Arianyan, Ehsan (36781884900); Buyya, Rajkumar (57194845546)","57217147405; 36781884900; 57194845546","A novel energy-aware resource management technique using joint VM and container consolidation approach for green computing in cloud data centers","2020","Simulation Modelling Practice and Theory","104","","102127","","","92","10.1016/j.simpat.2020.102127","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086469770&doi=10.1016%2Fj.simpat.2020.102127&partnerID=40&md5=f72c8e0ec1bad8f4b5962c1e204cb229","Cloud computing is being rapidly adopted for managing IT services as a notable solution due to diverse beneficiaries such as automatically optimized resource management as well as modern service delivery models. The container as a service has been recently introduced by cloud providers as a new service apart from traditional cloud services. Containers enable applications to run and deploy on isolated virtual space, and the operating system kernel is shared among them. Also, containerization has some attributes such as scalability, highly portable properties, and lightweight, for those reasons, it is applied for running isolated applications. Reducing energy consumption, as well as their CO<inf>2</inf> emissions, are great deals for cloud providers. In this direction, consolidation is recommended as a vital energy-aware approach in cloud data centers. Previously, independent virtual machine migration or container migration was proposed in the literature for green computing in cloud data centers. However, this paper proposes a new cloud resource management procedure based on a multi-criteria decision-making method that takes advantage of a joint virtual machine and container migration approach concurrently. The results of simulations using ContainerCloudsim simulator validates the applicability of the proposed approach which shows notable reductions in energy consumption, SLA violation, and number of migrations in comparison with the state-of-the-art algorithms. © 2020 Elsevier B.V.","Cloud computing; Consolidation; Containerization; Datacenter; Energy consumption; Resource management","Cloud computing; Computer aided software engineering; Containers; Decision making; Energy utilization; Information management; Natural resources management; Network security; Power management; Resource allocation; Virtual machine; Energy aware approaches; Multi-criteria decision making methods; Operating system kernel; Reducing energy consumption; Resource management; Resource management techniques; State-of-the-art algorithms; Virtual machine migrations; Green computing","Article","Final","","Scopus","2-s2.0-85086469770"
"Nath, S.B.; Addya, S.K.; Chakraborty, S.; Ghosh, S.K.","Nath, Shubha Brata (57215811414); Addya, Sourav Kanti (56538204500); Chakraborty, Sandip (54396851400); Ghosh, Soumya Kanti (24080140200)","57215811414; 56538204500; 54396851400; 24080140200","Green Containerized Service Consolidation in Cloud","2020","Conference Record - International Conference on Communications","2020-June","","9149173","","","11","10.1109/ICC40277.2020.9149173","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089422917&doi=10.1109%2FICC40277.2020.9149173&partnerID=40&md5=93dfb2db87d45fa70265ed7ad59db647","In the presence of latency sensitive geo-distributed applications, users require fast service for their queries. Cloud computing provides physical servers from its data center in order to process user requests. The cloud data center consumes a huge amount of energy due to lack of management of the data center servers as the container-based service consolidation is a nontrivial task. Since the containers require less resource footprint, consolidating it in servers might make resource availability sparse. In order to reduce the energy consumption of the cloud data center, we have proposed a green container-based consolidation of the services so that the maximum number of servers can be put into idle mode without affecting the application quality of experience. The service consolidation problem has been formulated as an optimization problem considering minimization of total energy consumption of the data center as the objective, and an algorithm named Energy Aware Service consolidation using baYesian optimization (EASY) has been proposed to solve the optimization. We have evaluated the EASY algorithm in simulation using python. The experimental results have shown that EASY improves the total energy consumption of the data centers. This improvement comes at the cost of a small increase of service response time as there exists a trade-off between energy consumption and service response time. © 2020 IEEE.","Bayesian optimization; Cloud Computing; Container; Energy Consumption; Service Consolidation","Computer software; Containers; Economic and social effects; Energy utilization; Power management; Quality of service; Application quality; Bayesian optimization; Cloud data centers; Distributed applications; Optimization problems; Resource availability; Service response time; Total energy consumption; Green computing","Conference paper","Final","","Scopus","2-s2.0-85089422917"
"Fieni, G.; Rouvoy, R.; Seinturier, L.","Fieni, Guillaume (57218400674); Rouvoy, Romain (23089521900); Seinturier, Lionel (6603303741)","57218400674; 23089521900; 6603303741","SmartWatts: Self-Calibrating Software-Defined Power Meter for Containers","2020","","","","9139675","479","488","48","10.1109/CCGrid49817.2020.00-45","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089066048&doi=10.1109%2FCCGrid49817.2020.00-45&partnerID=40&md5=370bdb1282e72f3eb6f23b05824cc59f","Fine-grained power monitoring of software activities becomes unavoidable to maximize the power usage efficiency of data centers. In particular, achieving an optimal scheduling of containers requires the deployment of software-defined power meters to go beyond the granularity of hardware power monitoring sensors, such as Power Distribution Units (PDU) or Intel's Running Average Power Limit (RAPL), to deliver power estimations of activities at the granularity of software containers. However, the definition of the underlying power models that estimate the power consumption remains a long and fragile process that is tightly coupled to the host machine.To overcome these limitations, this paper introduces SmartWatts: a lightweight power monitoring system that adopts online calibration to automatically adjust the CPU and DRAM power models in order to maximize the accuracy of runtime power estimations of containers. Unlike state-of-the-art techniques, SmartWatts does not require any a priori training phase or hardware equipment to configure the power models and can therefore be deployed on a wide range of machines including the latest power optimizations, at no cost. © 2020 IEEE.","Containers; Energy; Power model","Cluster computing; Containers; Optical parametric oscillators; Average power limit; Fine-grained power; On-line calibration; Optimal scheduling; Power distribution units; Power estimations; Power Optimization; State-of-the-art techniques; Multitasking","Conference paper","Final","All Open Access; Green Accepted Open Access; Green Open Access","Scopus","2-s2.0-85089066048"
"Silva-De-Souza, W.; Iranfar, A.; Bráulio, A.; Zapater, M.; Xavier-de-Souza, S.; Olcoz, K.; Atienza, D.","Silva-De-Souza, Wellington (57524832900); Iranfar, Arman (56939034300); Bráulio, Anderson (57216688911); Zapater, Marina (39462438800); Xavier-de-Souza, Samuel (59170006300); Olcoz, Katzalin (6603199337); Atienza, David A. (56259926900)","57524832900; 56939034300; 57216688911; 39462438800; 59170006300; 6603199337; 56259926900","Containergy-a container-based energy and performance profiling tool for next generation workloads","2020","Energies","13","9","2162","","","9","10.3390/en13092162","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084292416&doi=10.3390%2Fen13092162&partnerID=40&md5=cee24c4969a8083e9eef93f5ee2c1f83","Run-time profiling of software applications is key to energy efficiency. Even the most optimized hardware combined to an optimally designed software may become inefficient if operated poorly. Moreover, the diversification of modern computing platforms and broadening of their run-time configuration space make the task of optimally operating software ever more complex. With the growing financial and environmental impact of data center operation and cloud-based applications, optimal software operation becomes increasingly more relevant to existing and next-generation workloads. In order to guide software operation towards energy savings, energy and performance data must be gathered to provide a meaningful assessment of the application behavior under different system configurations, which is not appropriately addressed in existing tools. In this work we present Containergy, a new performance evaluation and profiling tool that uses software containers to perform application run-time assessment, providing energy and performance profiling data with negligible overhead (below 2%). It is focused on energy efficiency for next generation workloads. Practical experiments with emerging workloads, such as video transcoding and machine-learning image classification, are presented. The profiling results are analyzed in terms of performance and energy savings under a Quality-of-Service (QoS) perspective. For video transcoding, we verified that wrong choices in the configuration space can lead to an increase above 300% in energy consumption for the same task and operational levels. Considering the image classification case study, the results show that the choice of the machine-learning algorithm and model affect significantly the energy efficiency. Profiling datasets of AlexNet and SqueezeNet, which present similar accuracy, indicate that the latter represents 55.8% in energy saving compared to the former. © 2020 MDPI AG. All rights reserved.","DVFS; Energy profiling; Performance counters; Performance profiling; Software containers","Containers; Energy efficiency; Energy utilization; Environmental impact; Image classification; Learning algorithms; Machine learning; Quality of service; Space platforms; Video signal processing; Wave functions; Application behaviors; Cloud-based applications; Configuration space; Data center operations; Energy and performance profiling; Run-time configuration; Software applications; System configurations; Application programs","Article","Final","All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access","Scopus","2-s2.0-85084292416"
"Tan, B.; Ma, H.; Mei, Y.","Tan, Boxiong (57125400900); Ma, Hui (55723485400); Mei, Yi (37066417800)","57125400900; 55723485400; 37066417800","A Group Genetic Algorithm for Resource Allocation in Container-Based Clouds","2020","Lecture Notes in Computer Science","12102 LNCS","","","180","196","13","10.1007/978-3-030-43680-3_12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085013960&doi=10.1007%2F978-3-030-43680-3_12&partnerID=40&md5=e55f81914a5e36e74569d5fc9dc9a863","Containers have gain popularity because they support fast development and deployment of cloud-native software such as micro-services and server-less applications. Additionally, containers have low overhead, hence they save resources in cloud data centers. However, the difficulty of the Resource Allocation in Container-based clouds (RAC) is far beyond Virtual Machine (VM)-based clouds. The allocation task selects heterogeneous VMs to host containers and consolidate VMs to Physical Machines (PMs) simultaneously. Due to the high complexity, existing approaches use simple rule-based heuristics and meta-heuristics to solve the RAC problem. They either prone to stuck at local optima or have inherent defects in their indirect representations. To address these issues, we propose a novel group genetic algorithm (GGA) with a direct representation and problem-specific operators. This design has shown significantly better performance than the state-of-the-art algorithms in a wide range of test datasets. © 2020, Springer Nature Switzerland AG.","Cloud resource allocation; Container placement; Energy consumption; Group genetic algorithm","Application programs; Combinatorial optimization; Genetic algorithms; Resource allocation; Cloud data centers; High complexity; Local optima; Meta heuristics; Micro services; Rule-based heuristics; Specific operators; State-of-the-art algorithms; Containers","Conference paper","Final","","Scopus","2-s2.0-85085013960"
